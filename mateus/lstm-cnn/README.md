run the  prepare_data.py to preprocess and split the dataset into training, validation and test,
then run the create_corpus.py to create a file the tokenizer can use to tokenize
and run lstm-cnn.py to train the model
